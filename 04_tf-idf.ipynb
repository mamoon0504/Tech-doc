{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Representation\n",
    "\n",
    "**TF-IDF** stands for:\n",
    "- **Term Frequency (TF)**: Measures how frequently a term appears in a document. The more a term appears in a document, the higher its TF score.\n",
    "- **Inverse Document Frequency (IDF)**: Measures the importance of a term across the entire corpus. Words that appear in many documents (e.g., common words like \"the\" or \"is\") have a low IDF score, while words that are unique to fewer documents have a higher IDF score.\n",
    "\n",
    "The TF-IDF value of a term is calculated as:\n",
    "\n",
    "$$\n",
    "TF-IDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TF(t, d)**: Term frequency of term `t` in document `d`.\n",
    "- **IDF(t)**: Inverse document frequency of term `t`.\n",
    "\n",
    "##### How TF-IDF Works\n",
    "\n",
    "1. **Term Frequency (TF)**: TF measures the frequency of a term within a document. It is calculated as the ratio of the number of times a term occurs in a document to the total number of terms in that document. The goal is to emphasize words that are frequent within a document.\n",
    "\n",
    "![image.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lflj0Cz-X04bM2CKD9-ZTg.png)\n",
    "\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: IDF measures the rarity of a term across a collection of documents. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. The goal is to penalize words that are common across all documents.\n",
    "   \n",
    "![image.png](https://miro.medium.com/v2/resize:fit:1186/format:webp/1*d13YVVFq7YBXvbDkHaihQA.png)\n",
    "\n",
    "\n",
    "3. **Calculate TF-IDF**: Multiply the TF by the IDF for each word to get the final TF-IDF value, which represents how important a word is to a specific document.\n",
    "\n",
    "##### Advantages and Limitations of TF-IDF\n",
    "\n",
    "**Advantages**:\n",
    "- **Importance Weighting**: TF-IDF provides a measure of word importance, which helps reduce the influence of commonly occurring words that add little meaning.\n",
    "- **Simplicity**: Easy to implement and widely used in many text mining and information retrieval applications.\n",
    "\n",
    "**Limitations**:\n",
    "- **Lack of Semantic Understanding**: TF-IDF considers individual word frequencies without considering the semantic relationships between words.\n",
    "- **Data Sparsity**: Similar to the Bag of Words model, the TF-IDF representation can result in very sparse matrices for large vocabularies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Example of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Machine learning is a fascinating field\",\n",
    "    \"Data science and machine learning are closely related\",\n",
    "    \"Deep learning is a subfield of machine learning\",\n",
    "    \"Supervised learning involves labeled data\",\n",
    "    \"Unsupervised learning deals with unlabeled data\",\n",
    "    \"Feature engineering is crucial for model performance\",\n",
    "    \"Data preprocessing is an important step in machine learning\",\n",
    "    \"Natural language processing is a key area in AI\",\n",
    "    \"Hyperparameter tuning helps to optimize models\",\n",
    "    \"Model evaluation is necessary for understanding model accuracy\"\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents to create the TF-IDF representation\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to an array and print the vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Representation:\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Visualizing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the vocabulary and TF-IDF scores\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Sort the vocabulary and scores by TF-IDF score in descending order\n",
    "sorted_indices = np.argsort(-scores)\n",
    "sorted_vocabulary = vocabulary[sorted_indices]\n",
    "sorted_scores = scores[sorted_indices]\n",
    "\n",
    "# Select the top 10 words\n",
    "top_n = 10\n",
    "sorted_vocabulary = sorted_vocabulary[:top_n]\n",
    "sorted_scores = sorted_scores[:top_n]\n",
    "\n",
    "# Plot the TF-IDF scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_vocabulary, sorted_scores)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('TF-IDF Score')\n",
    "plt.title('Top 10 Words by TF-IDF Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 2\n",
    "\n",
    "# Initialize and fit the KMeans model\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get the cluster labels for each document\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Visualize the clustering using a scatter plot\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions with PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('K-Means Clustering of Documents (TF-IDF)')\n",
    "plt.show()\n",
    "\n",
    "# Print the cluster labels\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"Document {i}: Cluster {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Working with a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('movies_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.4)\n",
    "\n",
    "# Fit and transform the 'Cleaned_text' column to create the TF-IDF representation\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Cleaned_text'])\n",
    "\n",
    "# Normalizing the TF-IDF matrix\n",
    "tfidf_matrix_norm = normalize(tfidf_matrix)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_norm.toarray(), columns=terms)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3. Determine the Optimal Number of Clusters\n",
    "The `kneed` function is a part of the kneed Python library, which is used for finding the \"knee\" or \"elbow\" point in a dataset. The knee point represents an optimal value or a point of interest that helps determine the number of clusters or groups in data clustering or the appropriate value for other hyperparameters in various algorithms. It is commonly used in the context of the \"elbow method\" for determining the optimal number of clusters in K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator  # Optional for precise elbow detection\n",
    "\n",
    "# Calculate distortions for different values of k\n",
    "distortions = []\n",
    "K = range(1, 11)  # Test k from 1 to 10\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(tfidf_matrix_norm)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Determine the optimal k using the KneeLocator library\n",
    "knee_locator = KneeLocator(K, distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee_locator.knee\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of clusters for K-means\n",
    "num_clusters = 7  # This can be adjusted based on specific needs\n",
    "\n",
    "# Applying K-Means Clustering (this time we use some more parameters)\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=3000, n_init='auto')\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(tfidf_matrix_norm)\n",
    "\n",
    "# # Getting the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# # Displaying the first few cluster labels\n",
    "cluster_labels[:10]  # Showing the labels for the first 10 abstracts\n",
    "\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "dict = []\n",
    "for i in range(num_clusters):\n",
    "  print(\"%d\" % i, sep='', end=','),\n",
    "  for ind in order_centroids[i, :10]:\n",
    "    print(terms[ind], sep='', end=',')\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Translate to the Case\n",
    "Go to the case and perform k-means clustering on the news articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
