{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing\n",
    "\n",
    "In this notebook, we'll dive into **text pre-processing** in natural language processing. Text pre-processing is a critical step in preparing raw text data for analysis and is essential for many NLP tasks.\n",
    "\n",
    "##### Why Text Cleaning?\n",
    "\n",
    "Before we can perform any analysis or modeling, it's important to **clean the raw text**. Text cleaning involves transforming the unstructured, messy text data into a structured format that is easier to analyze. For example, we might need to remove **punctuation**, **stopwords** (like \"and\", \"the\"), and even convert words to **lowercase** to ensure consistency. The goal is to reduce noise and standardize the text to improve the quality of downstream analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic concepts\n",
    "Text cleaning is an essential preprocessing step for working with textual data. It involves a series of techniques to remove unwanted characters, standardize the text, and simplify the content. Below are some common text cleaning steps and an example using Python.\n",
    "\n",
    "**Various Forms of Text Cleaning**\n",
    "\n",
    "- **Lowercasing**: Converts all text to lowercase to ensure uniformity. This helps avoid treating words like \"Python\" and \"python\" as different.\n",
    "- **Removing Punctuation**: Eliminates punctuation marks such as periods, commas, and exclamation points, which often do not add meaningful value for analysis.\n",
    "- **Removing Numbers**: Removes numerical values if they are not relevant to the analysis, reducing noise in the dataset.\n",
    "- **Tokenization**: Splits the text into individual units or tokens, typically words, which makes it easier to analyze their frequency and context.\n",
    "- **Removing Stopwords**: Removes common words like \"is\", \"and\", \"the\" that do not carry significant meaning for many tasks.\n",
    "- **Lemmatization**: Converts words to their base or dictionary form, ensuring that variations of a word are treated as the same (e.g., \"running\" becomes \"run\").\n",
    "- **Stemming**: Similar to lemmatization, but more aggressive. Stemming reduces words to their root form by chopping off suffixes and prefixes, sometimes resulting in non-dictionary forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 The core concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk if not installed yet\n",
    "%pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download stopwords and WordNetLemmatizer from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "txt = \"Hello there! How are you doing today? The weather is gloomy and cold, but the Data Science track is awesome.\"\n",
    "\n",
    "# Convert text to lowercase\n",
    "cleaned_text = txt.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_text = cleaned_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Remove numbers\n",
    "cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "\n",
    "# Tokenize text\n",
    "tokens = cleaned_text.split()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Lemmatize tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Final cleaned text\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Regular Expressions (regex)\n",
    "\n",
    "**Regular expressions (regex)** are powerful tools for text processing and cleaning. Regex provides a concise way to search, match, and manipulate strings based on specific patterns. Regex can be used to find particular types of text such as dates, emails, or to clean unwanted characters.\n",
    "\n",
    "##### Common Regex Patterns\n",
    "- **Digits (`\\d`)**: Matches any digit.\n",
    "- **Word Characters (`\\w`)**: Matches any word character (letters, digits, and underscores).\n",
    "- **Whitespace (`\\s`)**: Matches any whitespace character (spaces, tabs, line breaks).\n",
    "- **Quantifiers (`+`, `*`, `{n}`)**: Specify the number of occurrences (e.g., `\\d+` matches one or more digits).\n",
    "- **Character Set (`[abc]`)**: Matches any character in the set (e.g., `[a-z]` matches any lowercase letter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text with different types of noise\n",
    "txt = \"Contact me at email@example.com or call 123-456-7890. Visit https://example.com for more info! :)\"\n",
    "\n",
    "# Extract phone numbers\n",
    "phone_numbers = re.findall(r'\\d{3}-\\d{3}-\\d{4}', txt)\n",
    "print(\"Phone Numbers:\", phone_numbers)\n",
    "\n",
    "# Extract phone numbers\n",
    "email_addresses = re.findall(r'\\S+@\\S+', txt)\n",
    "print(\"Email Addresses:\", email_addresses)\n",
    "\n",
    "# Remove special characters but keep the rest\n",
    "cleaned_text = re.sub(r'[^\\w\\s-]', '', txt)\n",
    "\n",
    "# Final cleaned text\n",
    "print(txt)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Working with a Dataframe\n",
    "In most cases you will load the data in a Dataframe. Let's take a look at the IMDb movie dataset that contains 9,000+ movies with plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('movies.csv')\n",
    "\n",
    "# Create a new Dataframe with Title and Plot of the first 10 movies\n",
    "df_text = df[['Title', 'Plot']].head(10)\n",
    "\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Converting to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation and non-word characters\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    \n",
    "    # Removing numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # you could add other options such as lemmatization, other stopwords\n",
    "    # ...\n",
    "\n",
    "    # Join tokens back into a sentence\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Preprocessing the Plots\n",
    "df_text['Cleaned_text'] = df_text['Plot'].apply(preprocess_text)\n",
    "\n",
    "# Examine results\n",
    "df_text[['Plot','Cleaned_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's scale up the text pre-processing\n",
    "df_text = df[['Title', 'Plot']].head(250)\n",
    "\n",
    "# Fill NaN\n",
    "df_text['Plot'] = df_text['Plot'].fillna(\"\")\n",
    "\n",
    "# Preprocessing the Plots\n",
    "df_text['Cleaned_text'] = df_text['Plot'].apply(preprocess_text)\n",
    "\n",
    "# Save for other notebooks\n",
    "df_text.to_csv('movies_cleaned.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Translate to the Case\n",
    "Go to the case and pre-process the news articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
